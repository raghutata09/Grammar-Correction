{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F4AMou8e3Q0Y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zOw5PAvADxH",
        "outputId": "aa463e3e-9e01-4773-c6c7-449157629857"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.8/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.8/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->flask-ngrok) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->flask-ngrok) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxdOkaYbAHwA",
        "outputId": "8b79625e-e30c-4a03-bb08-0bc3e3e26aba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ngrok is already the newest version (3.1.0).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2HK8LVeJFv2vB0Oxs4gpyBYwhDx_DekdQZkN4JT3dFeihNMx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLv1z2bsA4ld",
        "outputId": "87bfc9b6-8d97-4884-e459-1a2888ddaf98"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons==0.11.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTcqji5BlmN",
        "outputId": "e27ab1d7-4b67-4071-dd06-1c94ab1a819f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons==0.11.2 in /usr/local/lib/python3.8/dist-packages (0.11.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from  flask import Flask, jsonify,render_template, request\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#import matplotlib.ticker as ticker\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#import nltk.translate.gleu_score as gleu\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import  pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, LSTMCell\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n"
      ],
      "metadata": {
        "id": "-bMAw17s3kwD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder_DEcoder_Attention"
      ],
      "metadata": {
        "id": "F4AMou8e3Q0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tensorflow.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, output_dim, input_length, lstm_units, batch_size, embedding_matrix=None, em_trainable=False):\n",
        "    super().__init__()\n",
        "    self.lstm_units = lstm_units\n",
        "    self.batch_size = batch_size\n",
        "    self.en_hidden_state = 0\n",
        "    self.en_cell_state = 0\n",
        "    self.en_output = 0\n",
        "\n",
        "    #define layers\n",
        "    if embedding_matrix is None:\n",
        "      self.embedding = Embedding(input_dim=vocab_size, output_dim=output_dim, input_length=input_length, mask_zero=True, name=\"encoder_embedding\")\n",
        "    else:\n",
        "      self.embedding = Embedding(input_dim=vocab_size, output_dim=output_dim, input_length=input_length, mask_zero=True,\\\n",
        "                                 weights=[embedding_matrix], trainable=em_trainable, name='encoder_embedding')\n",
        "      \n",
        "    self.encoder = LSTM(units=self.lstm_units,return_state=True, return_sequences=True, name='encoder')\n",
        "\n",
        "  def call(self, input_sequences, states):\n",
        "\n",
        "    em_output = self.embedding(input_sequences)\n",
        "    self.en_output, self.en_hidden_state, self.en_cell_state = self.encoder(em_output, initial_state=states)\n",
        "\n",
        "    return self.en_output, self.en_hidden_state, self.en_cell_state\n",
        "\n",
        "  def initialize_states(self):\n",
        "\n",
        "    self.en_hidden_state = tensorflow.zeros(shape=(self.batch_size, self.lstm_units))\n",
        "    self.en_cell_state = tensorflow.zeros(shape=(self.batch_size, self.lstm_units))\n",
        "\n",
        "    return [self.en_hidden_state, self.en_cell_state]\n",
        "\n",
        "class Decoder(tensorflow.keras.Model):\n",
        "  def __init__(self, vocab_size, output_dim, input_length, dec_units, batch_sz, embedding_matrix=None, em_trainable=False, attention_type='luong'):\n",
        "    super().__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "    self.input_length = input_length\n",
        "\n",
        "    # Define Embedding Layer\n",
        "    if embedding_matrix is None:\n",
        "      self.embedding = Embedding(input_dim=vocab_size, output_dim=output_dim, input_length=input_length, name='decoder_embedding')\n",
        "    else:\n",
        "      self.embedding = Embedding(input_dim=vocab_size, output_dim=output_dim, input_length=input_length, \\\n",
        "                                 weights=[embedding_matrix], trainable=em_trainable)\n",
        "\n",
        "    #Final Dense layer on which softmax will be applied\n",
        "    self.fc = Dense(vocab_size+1)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = LSTMCell(self.dec_units)\n",
        "\n",
        "    # Sampler\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
        "                                                              None, self.batch_sz*[self.input_length], self.attention_type)\n",
        "\n",
        "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "\n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[self.input_length])\n",
        "    return outputs\n",
        "\n",
        "class Encoder_Decoder_Attention(tensorflow.keras.Model):\n",
        "\n",
        "  def __init__(self, en_vocab_size, en_out_dim, en_input_length, en_lstm, de_vocab_size, de_out_dim, de_input_length, de_lstm, batch_size,\\\n",
        "               attention='loung', en_embeddings=None, en_emb_trainable=False, de_embeddings=None, de_emb_trainable=False):\n",
        "\n",
        "    super().__init__()\n",
        "    #obj to encoder\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder = Encoder(en_vocab_size, en_out_dim, en_input_length, en_lstm, batch_size, en_embeddings, en_emb_trainable)\n",
        "    self.decoder = Decoder(de_vocab_size, de_out_dim, de_input_length, de_lstm, batch_size, de_embeddings, de_emb_trainable ,attention)\n",
        "\n",
        "\n",
        "  def call(self, data):\n",
        "\n",
        "    input_seq, target_seq = data[0], data[1]\n",
        "\n",
        "    #get encoder initial states\n",
        "    enc_states = self.encoder.initialize_states()\n",
        "\n",
        "    #encod input\n",
        "    enc_output, en_hidden_state, en_cell_state = self.encoder(input_seq, enc_states)\n",
        "\n",
        "    #set attention with encoder output\n",
        "    self.decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    #get initial ststes for decoder\n",
        "    de_initial_state = self.decoder.build_initial_state(self.batch_size, [en_hidden_state, en_cell_state], tensorflow.float32)\n",
        "\n",
        "    #get predictions from decoder\n",
        "    pred = self.decoder(target_seq, de_initial_state)\n",
        "\n",
        "    #return the final output\n",
        "    return pred.rnn_output"
      ],
      "metadata": {
        "id": "Q0TzvZCb3QYI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deploy"
      ],
      "metadata": {
        "id": "i18nZzHa3bcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Gl-fgJMX2ohw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#load pre requisities \n",
        "MAX_SEQ_LEN = 10\n",
        "\n",
        "train_encoder_inp = np.load('/content/drive/MyDrive/GrammarCorrection/model/train_encoder_inp.npy')\n",
        "train_decoder_inp = np.load('/content/drive/MyDrive/GrammarCorrection/model/train_decoder_inp.npy')\n",
        "train_decoder_out = np.load('/content/drive/MyDrive/GrammarCorrection/model/train_decoder_out.npy')\n",
        "\n",
        "encoder_embeddings = np.load('/content/drive/MyDrive/GrammarCorrection/model/encoder_embeddings.npy')\n",
        "decoder_embeddings = np.load('/content/drive/MyDrive/GrammarCorrection/model/decoder_embeddings.npy')\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/GrammarCorrection/model/tokenizer_encoder_word_level.pickle', 'rb') as fp:\n",
        "  tokenizer_encoder = pickle.load(fp)\n",
        "\n",
        "with open('/content/drive/MyDrive/GrammarCorrection/model/tokenizer_decoder_word_level.pickle','rb') as fp:\n",
        "  tokenizer_decoder = pickle.load(fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer and loss function\n",
        "\n",
        "optimizer = tensorflow.keras.optimizers.Adam()\n",
        "\n",
        "#defining custom loss function which will not consider loss for padded zeroes\n",
        "# code taken from attention assignment\n",
        "loss_object = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "  mask = tensorflow.math.logical_not(tensorflow.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tensorflow.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tensorflow.reduce_mean(loss_)\n"
      ],
      "metadata": {
        "id": "e-R-6034DEvY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training parameters\n",
        "inp_vocab_size = encoder_embeddings.shape[0]\n",
        "out_vocab_size = decoder_embeddings.shape[0] \n",
        "\n",
        "emb_dim = encoder_embeddings.shape[1]\n",
        "\n",
        "en_inp_len = MAX_SEQ_LEN\n",
        "de_inp_len = MAX_SEQ_LEN+1\n",
        "\n",
        "lstm_size=64\n",
        "batch_size=4\n",
        "\n",
        "#load model\n",
        "\n",
        "model = Encoder_Decoder_Attention(inp_vocab_size, emb_dim, en_inp_len, lstm_size,\\\n",
        "                                   out_vocab_size, emb_dim, de_inp_len, lstm_size,\\\n",
        "                                   batch_size,attention='bahdanau',\\\n",
        "                                   en_embeddings=encoder_embeddings, en_emb_trainable=True,\n",
        "                                   de_embeddings=decoder_embeddings, de_emb_trainable=True)\n"
      ],
      "metadata": {
        "id": "QwseWcoWDViv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function)\n",
        "\n",
        "model.train_on_batch([train_encoder_inp[:batch_size], train_decoder_inp[:batch_size]], train_decoder_out[:batch_size])\n",
        "model.load_weights('/content/drive/MyDrive/GrammarCorrection/checkpoints/Attention/en_de_bahdanau_trainable_w.h5')"
      ],
      "metadata": {
        "id": "niCNtvhdDZfw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def clean(text):\n",
        "    \"\"\"\n",
        "    takes string as input and\n",
        "    removes characters inside (),{},[] and <>\n",
        "    removes characters like -+@#^/|*(){}$~`\n",
        "    we not not removing ,.!-:;\"' as these characters are present in english language \n",
        "    \"\"\"\n",
        "    text = re.sub('<.*>', '', text)\n",
        "    text = re.sub('\\(.*\\)', '', text)\n",
        "    text = re.sub('\\[.*\\]', '', text)\n",
        "    text = re.sub('{.*}', '', text)\n",
        "    text = re.sub(\"[-+@#^/|*(){}$~<>=_%:;]\",\"\",text)\n",
        "    text = text.replace(\"\\\\\",\"\")\n",
        "    text = re.sub(\"\\[\",\"\",text)\n",
        "    text = re.sub(\"\\]\",\"\",text)\n",
        "    text = re.sub(\"[0-9]\",\"\",text)\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def beam_evaluate_sentence(sentences, model, beam_width):\n",
        "\n",
        "  inputs = tokenizer_encoder.texts_to_sequences(sentences)\n",
        "  inputs = pad_sequences(inputs, maxlen=MAX_SEQ_LEN, padding='post', truncating='post', value=0)\n",
        "  inputs = tensorflow.convert_to_tensor(inputs)\n",
        "\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result=''\n",
        "\n",
        "  #encoder\n",
        "  encoder_start_state = [tensorflow.zeros(shape=(inference_batch_size, lstm_size)), tensorflow.zeros(shape=(inference_batch_size, lstm_size))]\n",
        "  en_out, en_h, en_c = model.layers[0](inputs, encoder_start_state)\n",
        "\n",
        "  dec_h, dec_c = en_h, en_c\n",
        "\n",
        "  start_tokens = tensorflow.fill([inference_batch_size], tokenizer_decoder.word_index['<start>'])\n",
        "  end_token = tokenizer_decoder.word_index['<end>']\n",
        "\n",
        "  #tile encoder output\n",
        "  en_out_tiled = tfa.seq2seq.tile_batch(en_out, multiplier=beam_width)\n",
        "  model.layers[1].attention_mechanism.setup_memory(en_out_tiled)\n",
        "\n",
        "  #tile encoder hidden state\n",
        "  hidden_states = tfa.seq2seq.tile_batch([en_h, en_c], multiplier=beam_width)\n",
        "  decoder_initial_state = model.layers[1].rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tensorflow.float32)\n",
        "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_states)\n",
        "\n",
        "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(model.layers[1].rnn_cell, beam_width=beam_width, output_layer=model.layers[1].fc)\n",
        "\n",
        "  decoder_embedding_matrix = model.layers[1].embedding.variables[0]\n",
        "\n",
        "  outputs, final_state, seq_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token,\\\n",
        "                                                       initial_state=decoder_initial_state)\n",
        "  \n",
        "  final_outputs = tensorflow.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
        "  beam_scores = tensorflow.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
        "\n",
        "  return final_outputs.numpy(), beam_scores.numpy()\n",
        "\n",
        "def grammatical_error_correction(sentence, model):\n",
        "  result, beam_score = beam_evaluate_sentence(sentence, model, beam_width=3)\n",
        "  output = tokenizer_decoder.sequences_to_texts(result[0])\n",
        "\n",
        "  return output[0]"
      ],
      "metadata": {
        "id": "OlOgft7MDhJl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run"
      ],
      "metadata": {
        "id": "Of7EJBVhGZIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template,request\n",
        "from flask import Flask\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def hellword():\n",
        "  print('Hello word')\n",
        "  return jsonify({'message':'Hello World!!'})\n",
        "\n",
        "@app.route('/predict', methods=['post'])\n",
        "def predict():\n",
        "  print(\"Inside predict function\")\n",
        "  sent = request.get_json(force = True)\n",
        "  print(sent)\n",
        "  print(\"Input:  \", sent[\"1\"])\n",
        "  sent = sent[\"1\"]\n",
        "  print(\"---\"*30)\n",
        "  sent = decontracted(sent)\n",
        "  sent = clean(sent)\n",
        "  print(sent)\n",
        "  corrected = grammatical_error_correction([sent], model)\n",
        "\n",
        "  result = {'CORRECTED':corrected}\n",
        "  print(\"output:  \",result)\n",
        "  return jsonify(result)\n",
        "\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuBLePYMGCC1",
        "outputId": "22bff546-06a8-4703-b739-7fd673f8b775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://91e9-34-125-69-161.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [16/Dec/2022 07:51:34] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello word\n"
          ]
        }
      ]
    }
  ]
}